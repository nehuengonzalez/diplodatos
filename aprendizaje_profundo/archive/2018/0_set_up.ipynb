{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning - Part 0\n",
    "\n",
    "This notebook explains how to install all the preriquistes and libraries that you will need to run the following tutorials. If you can execute all the following cells, you are good to go.\n",
    "\n",
    "## Environment configuration\n",
    "\n",
    "There are two major package managers in Python: pip and conda. For this tutorial we will be using conda which, besides being a package manager is also useful as a version manager. There are two main ways to install conda: [Anaconda](https://conda.io/docs/install/quick.html) and [Miniconda](https://conda.io/miniconda.html).\n",
    "\n",
    "In order to install tensorflow we recommend following the [official documentation](https://www.tensorflow.org/install/install_linux#installing_with_anaconda). In particular, for the conda installation, they advise to use pip instead of conda as the only available Anaconda package for tensorflow is not actively mantained.\n",
    "\n",
    "All the available tensorflow versions (for both Python 2 and 3 and with CPU and GPU support) can be found [in this link](https://www.tensorflow.org/install/install_linux#top_of_page). For this course we will be using this tensorflow version for CPU https://storage.googleapis.com/tensorflow/linux/cpu/tensorflow-1.10.0-cp35-cp35m-linux_x86_64.whl and this one for GPU (for nabucodonosor) https://storage.googleapis.com/tensorflow/linux/gpu/tensorflow_gpu-1.10.0-cp35-cp35m-linux_x86_64.whl\n",
    "\n",
    "\n",
    "The commands to setup the environment are the following\n",
    "\n",
    "```\n",
    "$ wget https://repo.continuum.io/miniconda/Miniconda3-latest-Linux-x86_64.sh\n",
    "$ bash Miniconda3-latest-Linux-x86_64.sh\n",
    "$ conda create --name diplodatos python=3.5\n",
    "$ source activate diplodatos\n",
    "(diplodatos) $ conda install numpy scipy scikit-learn jupyter nb_conda keras\n",
    "(diplodatos) $ pip install --ignore-installed --upgrade YOUR_TENSORFLOW_URL\n",
    "(diplodatos) $ jupyter notebook\n",
    "```\n",
    "\n",
    "(note: it's quite important to install keras before tensorflow, as it overwrites the tf version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.2.2\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "\n",
    "print(keras.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optional libraries\n",
    "\n",
    "These are some optional libraries to download in order to see some visualizations. They take a while, so if you don't have good Internet connection or no time you can skip them.\n",
    "\n",
    "```\n",
    "# For the fasttext embeddings\n",
    "(diplodatos) $ pip install gensim\n",
    "# To visualize keras graphs\n",
    "(diplodatos) $ pip install pydot pydotplus\n",
    "(diplodatos) $ conda install graphviz matplotlib\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download the embeddings and the dataset\n",
    "\n",
    "### 2nd class\n",
    "\n",
    "The dataset we will use (MNIST) will be downloaded by Keras automatically the first time you use it. To save time, you can download it now running the next cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.datasets import mnist\n",
    "mnist.load_data();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assignment 1\n",
    "We will use the FastText embeddings. We provide a smaller version, filtered with only the words on the movie reviews dataset. You can download it from \n",
    "https://cs.famaf.unc.edu.ar/~mteruel/datasets/diplodatos/filtered_fastext_movie_review.pickle\n",
    "\n",
    "You can also download the original versions, if you want (most languages are available), from here https://github.com/facebookresearch/fastText/blob/master/pretrained-vectors.md. The English version is about 9GB."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you do not have the dataset, you can download and uncompress it from http://www.cs.cornell.edu/people/pabo/movie-review-data/review_polarity.tar.gz using the cell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "DIRECTORY=dataset\n",
    "if [ ! -d \"$DIRECTORY\" ]; then\n",
    "    # Control will enter here if dataset directory doesn't exist.\n",
    "    wget http://www.cs.cornell.edu/people/pabo/movie-review-data/review_polarity.tar.gz\n",
    "    mkdir \"$DIRECTORY\"\n",
    "    mv review_polarity.tar.gz \"$DIRECTORY\"\n",
    "    tar -xvf \"$DIRECTORY\"/review_polarity.tar.gz -C \"$DIRECTORY\"/\n",
    "fi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using the server"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tunneling and ssh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**How do you run a notebook in a remote machine?** You use an ssh connection with a port forwarding. This way, everything that goes to the port on the server machine (like a jupyter notebook) also goes to your localhost.\n",
    "\n",
    "It is likely that everyone will be using the same ports, so we recommend you to select a random number before connecting. The port on the ssh must be the same that you use to start the notebook.\n",
    "\n",
    "```\n",
    "$ ssh -L PORT:localhost:PORT USER@SERVER\n",
    "$ source activate diplodatos\n",
    "(diplodatos) $ jupyter notebook --port PORT --no-browser\n",
    "```\n",
    "\n",
    "Now you can use the notebook as if it were running on your computer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using slurm\n",
    "\n",
    "The Nabucodonosor server uses a queue system called slurm, which grants exclusive access to the CPU resources. You should enqueue everythin you do that takes more than 10 minutes!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set up\n",
    "\n",
    "1. Download the script https://raw.githubusercontent.com/MIREL-UNC/mirel-scripts/master/run_scripts/submit_job_slurm.sh\n",
    "\n",
    "2. Create a logs folder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Enqueue things\n",
    "\n",
    "To enqueue a job on slurm, first put your command in a file, for example command.txt\n",
    "```\n",
    "$ sbatch submit_job_slurm.sh commant.txt\n",
    "```\n",
    "\n",
    "The queue will assign your job a number JOBID. All the output of your process will be redirected to logs/JOBID.out and logs/JOBID.err"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Controlling things\n",
    "\n",
    "To see the state of the queue run `$ squeue`\n",
    "\n",
    "To cancel a job run `$ scancel JOBID`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Keras with GPUs "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you installed tensorflow with a GPU support, now it's a good time to check if it actually detects your devices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.10.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow\n",
    "print(tensorflow.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/device:GPU:0']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.python.client import device_lib\n",
    "\n",
    "def get_available_gpus():\n",
    "    local_device_protos = device_lib.list_local_devices()\n",
    "    return [x.name for x in local_device_protos if x.device_type == 'GPU']\n",
    "get_available_gpus()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the above gives an error, try setting the environment variables. You can add this to your .bashrc, the changes are only temporary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/opt/cuda/9.0/extras/CUPTI/lib64/:/opt/cuda/9.0/lib64:/opt/cudnn/v7.0/\n",
    "export CUDA_HOME=/opt/cuda/9.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Avoid using GPUs\n",
    "\n",
    "If all the GPUs are being used, you can still force Keras to use the CPU. For simple models this is still a very good option.\n",
    "\n",
    "The easiest way is to run you command with CUDA_VISIBLE_DEVICES=\"\". For example\n",
    "```\n",
    "(diplodatos) $ CUDA_VISIBLE_DEVICES=\"\" jupyter notebook --no-browser\n",
    "(diplodatos) $ CUDA_VISIBLE_DEVICES=\"\" exercise_1.py --experiment_name mlp_200\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:diplodatos]",
   "language": "python",
   "name": "conda-env-diplodatos-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
